---
title: "机器学习实战之线性回归"
date: 2021-01-29T23:13:43+08:00
draft: true
author: "YazhouTown"
description: "线性回归算法分析"
tags: [算法, AI]
categories: [机器学习实战]
---

<!--more-->

## 算法基本思想

### 回归分析

​	回归分析是一种预测性的建模技术，它研究的是因变量（目标）和自变量（预测器）之间的关系。这种技术通常用于预测分析，时间序列模型以及发现变量之间的因果关系。通常使用曲线/线来拟合数据点，目标是使曲线到数据点的距离差异最小

### 线性回归

![](https://gitee.com/isheihei/imagesRepo/raw/master/img/20201205125059.png)

使用均方误差作为损失函数，使用均方误差最小化目标函数的方法称为最小二乘法。

> 使用均方误差的原因：有十分好的几何意义，对应了常用的欧式距离。在线性回归中，就是找到一个直线，使得所有样本到直线的欧式距离最小。

损失代价函数定义为： 

![](https://gitee.com/isheihei/imagesRepo/raw/master/img/20201205125100.png)

展开后得到：

![](https://gitee.com/isheihei/imagesRepo/raw/master/img/20201205125101.png)

#### 损失函数求解

![](https://gitee.com/isheihei/imagesRepo/raw/master/img/20201205125102.png)

## 局部加权回归(Locally Weighted Regression, Loess)

### 简介

　　局部加权回归使一种非参数方法(Non-parametric)。在每次预测新样本时会重新训练临近的数据得到新参数值。意思是每次预测数据需要依赖训练训练集，所以每次估计的参数值是不确定的。

局部加权回归优点：

1. 需要预测的数据仅与到训练数据的距离有关，距离越近，关系越大，反之越小；
2. 可以有效避免欠拟合，减小了较远数据的干扰，仅与较近的数据有关。

### 与线性回归对比

- 线性回归的目标是最小化![](https://gitee.com/isheihei/imagesRepo/raw/master/img/20201205125103.png)
- Loess的目标是最小化![](https://gitee.com/isheihei/imagesRepo/raw/master/img/20201205125104.png)
- 在这里，我们给每个预测误差项一个权重，一般地![](https://gitee.com/isheihei/imagesRepo/raw/master/img/20201205125105.png)直观上的理解，就是对于某个预测点x，我们求解在x附近的线性回归函数，距离x越远的点，权重越低，对应误差项的就会越小
- 对于线性回归算法，一旦拟合出适合训练数据的参数θ，保存这些参数θi，对于之后的预测，不需要再使用原始训练数据集，所以是**参数学习算法**
- 对于局部加权线性回归算法，每次进行预测都需要全部的训练数据（每次进行的预测得到不同的参数θ），没有固定的参数θ，所以是**非参数算法**

```python
# 使用局部加权线性回归计算回归系数w
def lwlr(testPoint, xArr, yArr, k=1.0):
    """
    Parameters:
        testPoint - 测试样本点
        xArr - x数据集
        yArr - y数据集
        k - 高斯核的k,自定义参数
    Returns:
        testPoint * ws - 预测值
    """
    xMat = np.mat(xArr);
    yMat = np.mat(yArr).T
    m = np.shape(xMat)[0]
    weights = np.mat(np.eye((m)))  # 创建权重对角矩阵
    for j in range(m):  # 遍历数据集计算每个样本的权重
        diffMat = testPoint - xMat[j, :]
        weights[j, j] = np.exp(diffMat * diffMat.T / (-2.0 * k ** 2))
    xTx = xMat.T * (weights * xMat)
    if np.linalg.det(xTx) == 0.0:
        print("矩阵为奇异矩阵,不能求逆")
        return
    ws = xTx.I * (xMat.T * (weights * yMat))  # 计算回归系数，每条测试数据对应一个不同的回归系数
    return testPoint * ws

# 局部加权线性回归测试
def lwlrTest(testArr, xArr, yArr, k=1.0):
    """
    Parameters:
        testArr - 测试数据集
        xArr - x数据集
        yArr - y数据集
        k - 高斯核的k,自定义参数
    Returns:
        ws - 回归系数
    """
    m = np.shape(testArr)[0]  # 计算测试数据集大小
    yHat = np.zeros(m)
    for i in range(m):  # 对每个样本点进行预测
        yHat[i] = lwlr(testArr[i], xArr, yArr, k)
    return yHat

```

