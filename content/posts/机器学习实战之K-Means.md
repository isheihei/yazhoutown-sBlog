---
title: "机器学习实战之K Means"
date: 2021-01-29T23:05:39+08:00
draft: true
author: "YazhouTown"
description: "K-均值聚类（K-Means）算法分析"
tags: [算法, AI]
categories: [机器学习实战]
---

<!--more-->

## 算法基本思想

　聚类是一种无监督的学习，它将相似的对象归到同一簇中。聚类的方法几乎可以应用所有对象，簇内的对象越相似，聚类的效果就越好。K-means算法中的k表示的是聚类为k个簇，means代表取每一个聚类中数据值的均值作为该簇的中心，或者称为质心，即用每一个的类的质心对该簇进行描述。

　　聚类和分类最大的不同在于，分类的目标是事先已知的，而聚类则不一样，聚类事先不知道目标变量是什么，类别没有像分类那样被预先定义出来，所以，聚类有时也叫无监督学习。

　　聚类分析试图将相似的对象归入同一簇，将不相似的对象归为不同簇，那么，显然需要一种合适的相似度计算方法，我们已知的有很多相似度的计算方法，比如欧氏距离，余弦距离，汉明距离等。事实上，我们应该根据具体的应用来选取合适的相似度计算方法。

　　当然，任何一种算法都有一定的缺陷，没有一种算法时完美的，有的只是人类不断追求完美，不断创新的意志。K-means算法也有它的缺陷，但是我们可以通过一些后处理来得到更好的聚类结果，这些在后面都会一一降到。

　　K-means算法虽然比较容易实现，但是其可能收敛到局部最优解，且在大规模数据集上收敛速度相对较慢。

### K-Means伪代码

- 创建k个点作为起始质心，可以随机选择(位于数据边界内)
  - 当任意一个点的簇分配结果发生改变时
    - 对数据集中每一个点
      - 对每个质心
        - 计算质心与数据点之间的距离
      - 将数据点分配到距其最近的簇
    - 对每一个簇，计算簇中所有点的均值并将均值作为质心

## 核心算法

### 初始化质心

```python
#数据向量计算欧式距离    
def distEclud(vecA,vecB):
    return sqrt(sum(power(vecA-vecB,2)))

#随机初始化K个质心(质心满足数据边界之内)
def randCent(dataSet,k):
    #得到数据样本的维度
    n=shape(dataSet)[1]
    #初始化为一个(k,n)的矩阵
    centroids=mat(zeros((k,n)))
    #遍历数据集的每一维度
    for j in range(n):
        #得到该列数据的最小值
        minJ=min(dataSet[:,j])
        #得到该列数据的范围(最大值-最小值)
        rangeJ=float(max(dataSet[:,j])-minJ)
        #k个质心向量的第j维数据值随机为位于(最小值，最大值)内的某一值
        centroids[:,j]=minJ+rangeJ*random.rand(k,1)
    #返回初始化得到的k个质心向量
    return centroids
```

### K-Means均值聚类

```python
#k-均值聚类算法
#@dataSet:聚类数据集
#@k:用户指定的k个类
#@distMeas:距离计算方法，默认欧氏距离distEclud()
#@createCent:获得k个质心的方法，默认随机获取randCent()
def kMeans(dataSet,k,distMeas=distEclud,createCent=randCent):
    #获取数据集样本数
    m=shape(dataSet)[0]
    #初始化一个(m,2)的矩阵
    clusterAssment=mat(zeros((m,2)))
    #创建初始的k个质心向量
    centroids=createCent(dataSet,k)
    #聚类结果是否发生变化的布尔类型
    clusterChanged=True
    #只要聚类结果一直发生变化，就一直执行聚类算法，直至所有数据点聚类结果不变化
    while clusterChanged:
        #聚类结果变化布尔类型置为false
        clusterChanged=False
        #遍历数据集每一个样本向量
        for i in range(m):
            #初始化最小距离最正无穷；最小距离对应索引为-1
            minDist=inf;minIndex=-1
            #循环k个类的质心
            for j in range(k):
                #计算数据点到质心的欧氏距离
                distJI=distMeas(centroids[j,:],dataSet[i,:])
                #如果距离小于当前最小距离
                if distJI<minDist:
                    #当前距离定为当前最小距离；最小距离对应索引对应为j(第j个类)
                    minDist=distJI;minIndex=j
        #当前聚类结果中第i个样本的聚类结果发生变化：布尔类型置为true，继续聚类算法
        if clusterAssment[i,0] !=minIndex:clusterChanged=True
        #更新当前变化样本的聚类结果和平方误差
        clusterAssment[i,:]=minIndex,minDist**2
    #打印k-均值聚类的质心
    print centroids
    #遍历每一个质心
    for cent in range(k):
        #将数据集中所有属于当前质心类的样本通过条件过滤筛选出来
        ptsInClust=dataSet[nonzero(clusterAssment[:,0].A==cent)[0]]
        #计算这些数据的均值（axis=0：求列的均值），作为该类质心向量
        centroids[cent,:]=mean(ptsInClust,axis=0)
    #返回k个聚类，聚类结果及误差
    return centroids,clusterAssment
```

## 后处理提高聚类性能

有时候当我们观察聚类的结果图时，发现聚类的效果没有那么好，如上图所示，K-means算法在k值选取为3时的聚类结果，我们发现，算法能够收敛但效果较差。显然，这种情况的原因是，算法收敛到了局部最小值，而并不是全局最小值，局部最小值显然没有全局最小值的结果好。

　　那么，既然知道了算法已经陷入了局部最小值，如何才能够进一步提升K-means算法的效果呢？

　　一种用于度量聚类效果的指标是SSE，即误差平方和， 为所有簇中的全部数据点到簇中心的误差距离的平方累加和。SSE的值如果越小，表示数据点越接近于它们的簇中心，即质心，聚类效果也越好。因为，对误差取平方后，就会更加重视那些远离中心的数据点。

　　显然，我们知道了一种改善聚类效果的做法就是降低SSE，那么如何在保持簇数目不变的情况下提高簇的质量呢？

　　一种方法是：我们可以将具有最大SSE值得簇划分为两个簇（因为，SSE最大的簇一般情况下，意味着簇内的数据点距离簇中心较远），具体地，可以将最大簇包含的点过滤出来并在这些点上运行K-means算法，其中k设为2.

　　同时，当把最大的簇(上图中的下半部分)分为两个簇之后，为了保证簇的数目是不变的，我们可以再合并两个簇。具体地：

　　一方面我们可以合并两个最近的质心所对应的簇，即计算所有质心之间的距离，合并质心距离最近的两个质心所对应的簇。

　　另一方面，我们可以合并两个使得SSE增幅最小的簇，显然，合并两个簇之后SSE的值会有所上升，那么为了最好的聚类效果，应该尽可能使总的SSE值小，所以就选择合并两个簇后SSE涨幅最小的簇。具体地，就是计算合并任意两个簇之后的总得SSE，选取合并后最小的SSE对应的两个簇进行合并。这样，就可以满足簇的数目不变。

　　上面，是对已经聚类完成的结果进行改善的方法，在不改变k值的情况下，上述方法能够起到一定的作用，会使得聚类效果得到一定的改善。

## 二分K-means算法

　二分K-means算法首先将所有点作为一个簇，然后将簇一分为二。之后选择其中一个簇继续进行划分，选择哪一个簇取决于对其进行划分是否能够最大程度的降低SSE的值。上述划分过程不断重复，直至划分的簇的数目达到用户指定的值为止。

### 二分K-means算法的伪代码

- 将所有点看成一个簇

- 当簇数目小于k时

- 对于每一个簇

  - 计算总误差
  - 在给定的簇上面进行k-均值聚类（k=2）
  - 计算将该簇一分为二之后的总误差

- 选择使得总误差最小的簇进行划分（也可以选择SSE最大的簇进行划分）

  

